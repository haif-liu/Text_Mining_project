利用分词工具，进行中文分词处理。
常用的工具如下：

Stanford CoreNLP，支持汉语分词，java实现，代码开源，基于CRF，算法文档公开
官网：https://stanfordnlp.github.io/CoreNLP/

Paoding，Java开发，支持中文分词，需建立索引，查询效率较低
官网：http://code.google.com/p/paoding/

IKAnalyzer，开源轻量级的中文分词工具包
官网：http://code.google.com/p/ik-analyzer/

盘古分词，java实现，代码开源，词典管理功能完善，可以作为日常本地基本工具使用
官网：http://pangusegment.codeplex.com/



